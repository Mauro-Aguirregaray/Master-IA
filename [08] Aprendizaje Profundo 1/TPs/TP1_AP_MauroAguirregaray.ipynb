{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 Aprendizaje Profundo\n",
    "\n",
    "Se desea construir un sistema de recomendación de películas. Para esto se cuenta con un dataset de las puntuaciones que los usuarios han asignado a las peliculas disponibles.\n",
    "\n",
    "Link dataset: https://drive.google.com/file/d/1Og9H-8oqb3_Wo_WOakeAuRR_mwr922Ar/view?usp=sharing\n",
    "\n",
    "Para verificar la factibilidad del proyecto con datos válidos, se decide utilizar solamente las 200 películas con más votos y sobre eso los usuarios que han puntuado al menos 100 películas.\n",
    "\n",
    "1- Analizar el dataset para utilizar solamente las 200 películas con mayor cantidad de votos y los usuarios que hayan votado al menos 100 películas.\n",
    "\n",
    "2- A partir del dataset del punto 1, construir una única red neuronal que utilice una capa de embeddings para el id de usuario, una capa de embeddings para el id de película y al menos dos capas lineales que sea capaz de predecir el puntaje que cada usuario colocó a cada pelicula. Usar tecnicas de normalizacion en caso de ser necesario.\n",
    "\n",
    "3- Graficar las evoluciones de las funciones de costo en entrenamiento y validacion, como asi tambien las metricas de validacion. Explicar los resultados obtenidos.\n",
    "\n",
    "4- Construir una funcion capaz de recibir un usuario al azar, una cantidad \"p\" de películas que dicho usuario haya puntuado y verificar la predicción del modelo. Comparar con los puntajes reales contra los que el usuario asignó a dicha/s película/s.\n",
    "\n",
    "5- Contruir una funcion capaz de realizar una recomendación de película para un usuario determinado utilizando los embeddings de usuario o los embeddings de películas. Comprobar si la recomendación es correcta haciendo una predicción del puntuaje con la red neuronal.\n",
    "\n",
    "6- Con el mejor modelo obtenido del punto 2, elegir al menos 3 hiperparametros y aplicar algun metodo de tuneo. Explicar resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Análisis exploratorio\n",
    "\n",
    "Primero analicemos el dataset y preparemos los datos para el entrenamiento con aprendizaje profundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar el dataset\n",
    "movies_df = pd.read_csv(\"./datasets/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100836.000000</td>\n",
       "      <td>100836.000000</td>\n",
       "      <td>100836.000000</td>\n",
       "      <td>1.008360e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>326.127564</td>\n",
       "      <td>19435.295718</td>\n",
       "      <td>3.501557</td>\n",
       "      <td>1.205946e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>182.618491</td>\n",
       "      <td>35530.987199</td>\n",
       "      <td>1.042529</td>\n",
       "      <td>2.162610e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.281246e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>177.000000</td>\n",
       "      <td>1199.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.019124e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>325.000000</td>\n",
       "      <td>2991.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.186087e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>477.000000</td>\n",
       "      <td>8122.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.435994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>610.000000</td>\n",
       "      <td>193609.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.537799e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              userId        movieId         rating     timestamp\n",
       "count  100836.000000  100836.000000  100836.000000  1.008360e+05\n",
       "mean      326.127564   19435.295718       3.501557  1.205946e+09\n",
       "std       182.618491   35530.987199       1.042529  2.162610e+08\n",
       "min         1.000000       1.000000       0.500000  8.281246e+08\n",
       "25%       177.000000    1199.000000       3.000000  1.019124e+09\n",
       "50%       325.000000    2991.000000       3.500000  1.186087e+09\n",
       "75%       477.000000    8122.000000       4.000000  1.435994e+09\n",
       "max       610.000000  193609.000000       5.000000  1.537799e+09"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>3.0</td>\n",
       "      <td>964982400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964980868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964984041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964984100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931\n",
       "5       1       70     3.0  964982400\n",
       "6       1      101     5.0  964980868\n",
       "7       1      110     4.0  964982176\n",
       "8       1      151     5.0  964984041\n",
       "9       1      157     5.0  964984100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos en frente a un dataset con tan solo 4 features y una gran cantidad de entradas (100836). En particular se observan dos features \"userID\" y \"movieID\" que hacen referencia a usuarios X y películas Y respectivamente. Es un caso donde estos features si bien son numéricos se refieren más correctamente a variables categoricas, por lo que el uso de la herramienta de embeddings resulta de mucha utilidad.\n",
    "\n",
    "Además de estos features \"rating\" es un flotante que va de 0 a 5 (y el feature objetivo en este caso) y \"timestamp\" parece ser una medida de cuando fue subida la review que a nivel lógico no parecería aportar mucha información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo a lo que pide el problema debemos filtrar el dataset para quedarnos solo con las 200 películas más votadas y sobre esas con los usuarios que hayan votado más de 100 películas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 25764 entries, 0 to 100452\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   userId     25764 non-null  int64  \n",
      " 1   movieId    25764 non-null  int64  \n",
      " 2   rating     25764 non-null  float64\n",
      " 3   timestamp  25764 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 1006.4 KB\n"
     ]
    }
   ],
   "source": [
    "#Filtrar las 200 películas más rateadas\n",
    "\n",
    "#Cuento las instancias de cada película\n",
    "value_counts_movies = movies_df[\"movieId\"].value_counts()\n",
    "\n",
    "#Guardo los id de las 200 películas con más ratings\n",
    "top_200_movies = value_counts_movies.head(200).index\n",
    "\n",
    "#Filtro el dataset para que solo contenga estas películas\n",
    "movies_df = movies_df[movies_df[\"movieId\"].isin(top_200_movies)]\n",
    "\n",
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId\n",
       "356     329\n",
       "318     317\n",
       "296     307\n",
       "593     279\n",
       "2571    278\n",
       "       ... \n",
       "3897     83\n",
       "1101     83\n",
       "16       82\n",
       "788      82\n",
       "1584     82\n",
       "Name: count, Length: 200, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chequeo que mi dataset solo contenga 200 películas diferentes\n",
    "movies_df[\"movieId\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misma idea pero para los usuarios ahora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrar los 100 usuarios que más reviews tienen\n",
    "\n",
    "#Cuento la cantidad de reviews por user\n",
    "value_counts_users = movies_df[\"userId\"].value_counts()\n",
    "\n",
    "#Guardo los id de las 100 users con más reviewss\n",
    "top_users = value_counts_users[value_counts_users>99].index\n",
    "\n",
    "#Filtro el dataset para que solo contenga estos usuarios\n",
    "movies_df = movies_df[movies_df[\"userId\"].isin(top_users)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId\n",
       "414    194\n",
       "599    189\n",
       "68     185\n",
       "480    177\n",
       "474    173\n",
       "      ... \n",
       "200    104\n",
       "453    103\n",
       "166    102\n",
       "603    102\n",
       "354    100\n",
       "Name: count, Length: 63, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chequeo que no hayan quedado usuarios con menos de 100 reviews dentro del dataset\n",
    "movies_df[\"userId\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8329 entries, 1772 to 100452\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   userId     8329 non-null   int64  \n",
      " 1   movieId    8329 non-null   int64  \n",
      " 2   rating     8329 non-null   float64\n",
      " 3   timestamp  8329 non-null   int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 325.4 KB\n"
     ]
    }
   ],
   "source": [
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Separación Train-Test-Validation\n",
    "\n",
    "Separamos en train test el dataset, también me quito timestamp porque considero que no aporta información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo user_id para modelo con embeddings\n",
    "user_id = movies_df['userId']\n",
    "\n",
    "# Guardo movie_id para modelo con embeddings\n",
    "movie_id = movies_df['movieId']\n",
    "\n",
    "X = movies_df.drop(columns=[\"rating\",\"timestamp\"]).values\n",
    "y = movies_df[\"rating\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos en los sets de entrenamiento y evaluación\n",
    "X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(X, y,np.arange(X.shape[0]), test_size = 0.20, random_state = 42)\n",
    "\n",
    "#Separamos sobre el set de entrenamiento, un subset de validación\n",
    "X_train, X_valid, y_train, y_valid, train_idx, valid_idx = train_test_split(X_train, y_train, train_idx, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8329"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "n_valid = X_valid.shape[0]\n",
    "\n",
    "#Chequeamos que el total de datos está comprendido en la separación\n",
    "n_test+n_train+n_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo user id a indices (idx) consecutivos para utilizar embeddings\n",
    "user_id_to_idx = {value:i for i,value in enumerate(user_id.unique())}\n",
    "\n",
    "# Transformo user id a indices (idx) consecutivos para utilizar embeddings\n",
    "movie_id_to_idx = {value:i for i,value in enumerate(movie_id.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector de user_idx en el dataset\n",
    "user_idx = np.array([user_id_to_idx[value] for value in user_id])\n",
    "\n",
    "# Vector de movie_idx en el dataset\n",
    "movie_idx = np.array([movie_id_to_idx[value] for value in movie_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divido el vector user_idx en entrenamiento y validación\n",
    "user_idx_train = user_idx[train_idx]\n",
    "user_idx_valid = user_idx[valid_idx]\n",
    "\n",
    "# Divido el vector movie_idx en entrenamiento y validación\n",
    "movie_idx_train = movie_idx[train_idx]\n",
    "movie_idx_valid = movie_idx[valid_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch necesita de una clase de dataset que extienda de torch.utils.data.Dataset\n",
    "# Esta clase dataset debe sobreescribir los métodos init, len y getitem\n",
    "class MyDatasetWithEmbddings(Dataset):\n",
    "\n",
    "  #__init__ guarda el dataset en una variable de clase\n",
    "  def __init__(self, user_idx, movie_idx, y): #no tengo features más alla de los embeddings\n",
    "    self.user_idx = user_idx\n",
    "    self.movie_idx = movie_idx\n",
    "    self.y = y\n",
    "\n",
    "  # __len__ define el comportamiento de la función len() sobre el objeto\n",
    "  def __len__(self):\n",
    "    return self.user_idx.shape[0]\n",
    "\n",
    "  # __getitem__ define el comportamiento de los []\n",
    "  def __getitem__(self, idx):\n",
    "    return  self.user_idx[idx], self.movie_idx[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el dataset de entrenamiento\n",
    "df_train = MyDatasetWithEmbddings(user_idx_train, movie_idx_train, y_train)\n",
    "# Creo el dataset de validación\n",
    "df_valid = MyDatasetWithEmbddings(user_idx_valid, movie_idx_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch utiliza DataLoader para entregar los dataset de a batches\n",
    "train_dataloader = DataLoader(df_train, batch_size = 64, shuffle= True)\n",
    "valid_dataloader = DataLoader(df_valid, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que definir la arquitectura de nuestra red neuronal. De acuerdo a lo que pide el ejercicio tenemos que definir una capa de embedding por feature y luego dos capas lineales.\n",
    "\n",
    "En particular y como no tengo referencia del problema voy a usar una regla empirica para definir la dimensionalidad de la capa de embeddings, en particular tomar aproximadamente la raíz cuadrada de la cantidad de instancias distintas. Esto es 14 para moviesId y 8 para userId.\n",
    "\n",
    "En cuanto a las capas lineales mantendré las estándar mostradas en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitectura con embeddings\n",
    "class NNetWithEmbeddings(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.embeddings_user = torch.nn.Embedding(num_embeddings=63, embedding_dim=8)\n",
    "    self.embeddings_movie = torch.nn.Embedding(num_embeddings=200, embedding_dim=14)\n",
    "    self.linear_1 = torch.nn.Linear(in_features=14+8, out_features=200, bias=True)\n",
    "    self.relu_1 = torch.nn.ReLU()\n",
    "    self.linear_2 = torch.nn.Linear(in_features = 200, out_features=100, bias=True)\n",
    "    self.relu_2 = torch.nn.ReLU()\n",
    "    self.output = torch.nn.Linear(in_features = 100, out_features= 1, bias=True)\n",
    "\n",
    "  def forward(self, user_idx, movie_idx):\n",
    "    embeddings_outputs_user = self.embeddings_user(user_idx)\n",
    "    embeddings_outputs_movie =self.embeddings_movie(movie_idx)\n",
    "    x = torch.cat([embeddings_outputs_user, embeddings_outputs_movie], dim=1)\n",
    "    x = self.linear_1(x)\n",
    "    x = self.relu_1(x)\n",
    "    x = self.linear_2(x)\n",
    "    x = self.relu_2(x)\n",
    "    x = self.output(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializo mi red\n",
    "nnnetWithEmbeddings = NNetWithEmbeddings()\n",
    "nnnetWithEmbeddings = nnnetWithEmbeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.MSELoss()\n",
    "# Optimizador con regularización L2 (parámetro weight_decay)\n",
    "optimizer = torch.optim.Adam(nnnetWithEmbeddings.parameters(), lr=0.001, weight_decay=0.00001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 0 | Train/Valid loss: 12107.455 / 279.382 | Train/Valid mse: 12107.454 / 279.382\n",
      " Epoch 1 | Train/Valid loss: 216.692 / 191.593 | Train/Valid mse: 216.692 / 191.593\n",
      " Epoch 2 | Train/Valid loss: 146.488 / 141.449 | Train/Valid mse: 146.488 / 141.449\n",
      " Epoch 3 | Train/Valid loss: 111.012 / 148.344 | Train/Valid mse: 111.012 / 148.344\n",
      " Epoch 4 | Train/Valid loss: 98.445 / 103.008 | Train/Valid mse: 98.445 / 103.008\n",
      " Epoch 5 | Train/Valid loss: 79.785 / 101.061 | Train/Valid mse: 79.785 / 101.061\n",
      " Epoch 6 | Train/Valid loss: 70.160 / 118.698 | Train/Valid mse: 70.160 / 118.698\n",
      " Epoch 7 | Train/Valid loss: 66.469 / 80.960 | Train/Valid mse: 66.469 / 80.960\n",
      " Epoch 8 | Train/Valid loss: 59.602 / 75.755 | Train/Valid mse: 59.602 / 75.755\n",
      " Epoch 9 | Train/Valid loss: 56.552 / 58.369 | Train/Valid mse: 56.552 / 58.369\n",
      " Epoch 10 | Train/Valid loss: 48.025 / 53.096 | Train/Valid mse: 48.025 / 53.096\n",
      " Epoch 11 | Train/Valid loss: 41.988 / 54.418 | Train/Valid mse: 41.988 / 54.418\n",
      " Epoch 12 | Train/Valid loss: 52.284 / 53.940 | Train/Valid mse: 52.284 / 53.940\n",
      " Epoch 13 | Train/Valid loss: 40.644 / 58.614 | Train/Valid mse: 40.644 / 58.614\n",
      " Epoch 14 | Train/Valid loss: 34.705 / 43.136 | Train/Valid mse: 34.705 / 43.136\n",
      " Epoch 15 | Train/Valid loss: 59.270 / 78.060 | Train/Valid mse: 59.270 / 78.060\n",
      " Epoch 16 | Train/Valid loss: 40.919 / 53.607 | Train/Valid mse: 40.919 / 53.607\n",
      " Epoch 17 | Train/Valid loss: 42.794 / 45.665 | Train/Valid mse: 42.794 / 45.665\n",
      " Epoch 18 | Train/Valid loss: 60.263 / 34.396 | Train/Valid mse: 60.263 / 34.396\n",
      " Epoch 19 | Train/Valid loss: 125.290 / 426.408 | Train/Valid mse: 125.290 / 426.408\n",
      " Epoch 20 | Train/Valid loss: 272.825 / 60.348 | Train/Valid mse: 272.825 / 60.348\n",
      " Epoch 21 | Train/Valid loss: 71.196 / 71.324 | Train/Valid mse: 71.196 / 71.324\n",
      " Epoch 22 | Train/Valid loss: 67.368 / 90.511 | Train/Valid mse: 67.368 / 90.511\n",
      " Epoch 23 | Train/Valid loss: 90.214 / 69.370 | Train/Valid mse: 90.214 / 69.370\n",
      " Epoch 24 | Train/Valid loss: 93.728 / 65.794 | Train/Valid mse: 93.728 / 65.794\n",
      " Epoch 25 | Train/Valid loss: 128.423 / 34.779 | Train/Valid mse: 128.423 / 34.779\n",
      " Epoch 26 | Train/Valid loss: 161.638 / 1115.450 | Train/Valid mse: 161.638 / 1115.450\n",
      " Epoch 27 | Train/Valid loss: 10038.750 / 131509.677 | Train/Valid mse: 10038.751 / 131509.672\n",
      " Epoch 28 | Train/Valid loss: 33506.315 / 2530.983 | Train/Valid mse: 33506.312 / 2530.983\n",
      " Epoch 29 | Train/Valid loss: 609.994 / 847.886 | Train/Valid mse: 609.994 / 847.885\n",
      " Epoch 30 | Train/Valid loss: 242.777 / 175.280 | Train/Valid mse: 242.777 / 175.280\n",
      " Epoch 31 | Train/Valid loss: 151.136 / 86.854 | Train/Valid mse: 151.136 / 86.854\n",
      " Epoch 32 | Train/Valid loss: 124.115 / 117.861 | Train/Valid mse: 124.115 / 117.861\n",
      " Epoch 33 | Train/Valid loss: 69.576 / 54.818 | Train/Valid mse: 69.576 / 54.818\n",
      " Epoch 34 | Train/Valid loss: 48.664 / 95.742 | Train/Valid mse: 48.664 / 95.742\n",
      " Epoch 35 | Train/Valid loss: 33.265 / 21.184 | Train/Valid mse: 33.265 / 21.184\n",
      " Epoch 36 | Train/Valid loss: 33.954 / 32.096 | Train/Valid mse: 33.954 / 32.096\n",
      " Epoch 37 | Train/Valid loss: 35.068 / 347.235 | Train/Valid mse: 35.068 / 347.235\n",
      " Epoch 38 | Train/Valid loss: 112.764 / 50.198 | Train/Valid mse: 112.764 / 50.198\n",
      " Epoch 39 | Train/Valid loss: 30.164 / 71.646 | Train/Valid mse: 30.164 / 71.646\n",
      " Epoch 40 | Train/Valid loss: 71.183 / 16.276 | Train/Valid mse: 71.183 / 16.276\n",
      " Epoch 41 | Train/Valid loss: 380.098 / 49.814 | Train/Valid mse: 380.098 / 49.814\n",
      " Epoch 42 | Train/Valid loss: 45.275 / 15.933 | Train/Valid mse: 45.275 / 15.933\n",
      " Epoch 43 | Train/Valid loss: 25.820 / 50.676 | Train/Valid mse: 25.820 / 50.676\n",
      " Epoch 44 | Train/Valid loss: 1045.781 / 461.483 | Train/Valid mse: 1045.781 / 461.483\n",
      " Epoch 45 | Train/Valid loss: 169.602 / 24.027 | Train/Valid mse: 169.602 / 24.027\n",
      " Epoch 46 | Train/Valid loss: 40.056 / 71.354 | Train/Valid mse: 40.056 / 71.354\n",
      " Epoch 47 | Train/Valid loss: 20.430 / 17.448 | Train/Valid mse: 20.430 / 17.448\n",
      " Epoch 48 | Train/Valid loss: 24.088 / 178.240 | Train/Valid mse: 24.088 / 178.240\n",
      " Epoch 49 | Train/Valid loss: 162.283 / 35.842 | Train/Valid mse: 162.283 / 35.842\n",
      " Epoch 50 | Train/Valid loss: 51.610 / 155.182 | Train/Valid mse: 51.610 / 155.182\n",
      " Epoch 51 | Train/Valid loss: 186.324 / 24.571 | Train/Valid mse: 186.324 / 24.571\n",
      " Epoch 52 | Train/Valid loss: 85.143 / 304.159 | Train/Valid mse: 85.143 / 304.159\n",
      " Epoch 53 | Train/Valid loss: 141.953 / 25.454 | Train/Valid mse: 141.953 / 25.454\n",
      " Epoch 54 | Train/Valid loss: 95.135 / 24.336 | Train/Valid mse: 95.135 / 24.336\n",
      " Epoch 55 | Train/Valid loss: 41.390 / 138.778 | Train/Valid mse: 41.390 / 138.778\n",
      " Epoch 56 | Train/Valid loss: 160.829 / 20.903 | Train/Valid mse: 160.829 / 20.903\n",
      " Epoch 57 | Train/Valid loss: 166.169 / 218.053 | Train/Valid mse: 166.169 / 218.053\n",
      " Epoch 58 | Train/Valid loss: 305.787 / 27.405 | Train/Valid mse: 305.787 / 27.405\n",
      " Epoch 59 | Train/Valid loss: 32.348 / 99.359 | Train/Valid mse: 32.348 / 99.359\n",
      " Epoch 60 | Train/Valid loss: 71.984 / 19.432 | Train/Valid mse: 71.984 / 19.432\n",
      " Epoch 61 | Train/Valid loss: 66.042 / 43.684 | Train/Valid mse: 66.042 / 43.684\n",
      " Epoch 62 | Train/Valid loss: 144.813 / 36.496 | Train/Valid mse: 144.813 / 36.496\n",
      " Epoch 63 | Train/Valid loss: 114.381 / 50.254 | Train/Valid mse: 114.381 / 50.254\n",
      " Epoch 64 | Train/Valid loss: 85.199 / 56.149 | Train/Valid mse: 85.199 / 56.149\n",
      " Epoch 65 | Train/Valid loss: 127.953 / 20.960 | Train/Valid mse: 127.953 / 20.960\n",
      " Epoch 66 | Train/Valid loss: 41.027 / 383.519 | Train/Valid mse: 41.027 / 383.519\n",
      " Epoch 67 | Train/Valid loss: 339.137 / 130.019 | Train/Valid mse: 339.137 / 130.019\n",
      " Epoch 68 | Train/Valid loss: 126.154 / 243.907 | Train/Valid mse: 126.154 / 243.907\n",
      " Epoch 69 | Train/Valid loss: 73.289 / 35.602 | Train/Valid mse: 73.289 / 35.602\n",
      " Epoch 70 | Train/Valid loss: 31.817 / 19.113 | Train/Valid mse: 31.817 / 19.113\n",
      " Epoch 71 | Train/Valid loss: 64.880 / 822.226 | Train/Valid mse: 64.880 / 822.226\n",
      " Epoch 72 | Train/Valid loss: 317.855 / 250.735 | Train/Valid mse: 317.855 / 250.735\n",
      " Epoch 73 | Train/Valid loss: 134.245 / 40.841 | Train/Valid mse: 134.245 / 40.841\n",
      " Epoch 74 | Train/Valid loss: 138.525 / 51.864 | Train/Valid mse: 138.525 / 51.864\n",
      " Epoch 75 | Train/Valid loss: 41.385 / 88.709 | Train/Valid mse: 41.385 / 88.709\n",
      " Epoch 76 | Train/Valid loss: 99.292 / 134.690 | Train/Valid mse: 99.292 / 134.690\n",
      " Epoch 77 | Train/Valid loss: 305.732 / 40.163 | Train/Valid mse: 305.732 / 40.163\n",
      " Epoch 78 | Train/Valid loss: 30.707 / 28.561 | Train/Valid mse: 30.707 / 28.561\n",
      " Epoch 79 | Train/Valid loss: 27.561 / 30.176 | Train/Valid mse: 27.561 / 30.176\n",
      " Epoch 80 | Train/Valid loss: 114.395 / 430.568 | Train/Valid mse: 114.395 / 430.568\n",
      " Epoch 81 | Train/Valid loss: 205.747 / 43.497 | Train/Valid mse: 205.747 / 43.497\n",
      " Epoch 82 | Train/Valid loss: 77.737 / 304.388 | Train/Valid mse: 77.737 / 304.388\n",
      " Epoch 83 | Train/Valid loss: 120.371 / 53.021 | Train/Valid mse: 120.371 / 53.021\n",
      " Epoch 84 | Train/Valid loss: 55.565 / 38.685 | Train/Valid mse: 55.565 / 38.685\n",
      " Epoch 85 | Train/Valid loss: 259.471 / 604.570 | Train/Valid mse: 259.471 / 604.570\n",
      " Epoch 86 | Train/Valid loss: 413.737 / 212.419 | Train/Valid mse: 413.736 / 212.419\n",
      " Epoch 87 | Train/Valid loss: 137.030 / 57.157 | Train/Valid mse: 137.030 / 57.157\n",
      " Epoch 88 | Train/Valid loss: 204.671 / 142.358 | Train/Valid mse: 204.671 / 142.358\n",
      " Epoch 89 | Train/Valid loss: 72.478 / 146.609 | Train/Valid mse: 72.478 / 146.609\n",
      " Epoch 90 | Train/Valid loss: 106.634 / 39.689 | Train/Valid mse: 106.634 / 39.689\n",
      " Epoch 91 | Train/Valid loss: 49.547 / 134.101 | Train/Valid mse: 49.547 / 134.101\n",
      " Epoch 92 | Train/Valid loss: 110.722 / 27.477 | Train/Valid mse: 110.722 / 27.477\n",
      " Epoch 93 | Train/Valid loss: 87.705 / 58.639 | Train/Valid mse: 87.705 / 58.639\n",
      " Epoch 94 | Train/Valid loss: 79.522 / 90.681 | Train/Valid mse: 79.521 / 90.681\n",
      " Epoch 95 | Train/Valid loss: 99.237 / 265.491 | Train/Valid mse: 99.237 / 265.491\n",
      " Epoch 96 | Train/Valid loss: 425.663 / 1202.837 | Train/Valid mse: 425.663 / 1202.837\n",
      " Epoch 97 | Train/Valid loss: 34774.998 / 4607.705 | Train/Valid mse: 34775.000 / 4607.705\n",
      " Epoch 98 | Train/Valid loss: 1198.322 / 285.002 | Train/Valid mse: 1198.322 / 285.002\n",
      " Epoch 99 | Train/Valid loss: 303.866 / 203.067 | Train/Valid mse: 303.866 / 203.067\n"
     ]
    }
   ],
   "source": [
    "# Mini-Batch\n",
    "\n",
    "# cantidad de epochs\n",
    "epochs = 100\n",
    "\n",
    "train_loss_by_epoch=[]\n",
    "valid_loss_by_epoch=[]\n",
    "\n",
    "# Doble loop algoritmo Mini-Batch\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  ############################################\n",
    "  ## Entrenamiento\n",
    "  ############################################\n",
    "  nnnetWithEmbeddings.train(True)\n",
    "\n",
    "  epoch_loss = 0\n",
    "  epoch_y_hat = []\n",
    "  epoch_y = []\n",
    "  \n",
    "  for i,data in enumerate(train_dataloader):\n",
    "    # Obtengo los datos del batch de entrenamiento\n",
    "    embed_user_batch, embed_movie_batch, y_batch = data\n",
    "    # Copio el batch al dispositivo donde entreno la red neuronal\n",
    "    embed_user_batch = embed_user_batch.to(device).int()\n",
    "    embed_movie_batch = embed_movie_batch.to(device).int()\n",
    "    y_batch = y_batch.to(device).float().reshape(-1, 1)\n",
    "\n",
    "    # Paso forward\n",
    "    # Limpio optimizer para empezar un nuevo cálculo de gradiente\n",
    "    optimizer.zero_grad()\n",
    "    nnet_output = nnnetWithEmbeddings(embed_user_batch, embed_movie_batch)\n",
    "    y_batch_hat = nnet_output\n",
    "    \n",
    "    # Calculo el loss\n",
    "    loss = loss_function(nnet_output, y_batch)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Actualizar los parámetros\n",
    "    optimizer.step()\n",
    "\n",
    "    # Almaceno los valores reales y mis predicciones para cálcular las métricas\n",
    "    epoch_y += list(y_batch.detach().cpu().numpy())\n",
    "    epoch_y_hat += list(y_batch_hat.detach().cpu().numpy())\n",
    "    # Acumulo la loss del batch\n",
    "    epoch_loss = epoch_loss + loss.item()\n",
    "\n",
    "    # Almaceno la loss de la epoch para graficar\n",
    "  train_loss_by_epoch.append(epoch_loss)\n",
    "  # Cálculo la métrica de la epoch\n",
    "  train_mse = mean_squared_error(epoch_y, epoch_y_hat)\n",
    "\n",
    "  ############################################\n",
    "  ## Validación\n",
    "  ############################################\n",
    "  # Desactivo el cálculo de gradiente para validación\n",
    "  nnnetWithEmbeddings.train(False)\n",
    "\n",
    "  valid_epoch_loss = 0\n",
    "  valid_epoch_y_hat = []\n",
    "  valid_epoch_y = []\n",
    "\n",
    "  for i,data in enumerate(valid_dataloader):\n",
    "    # Obtengo los datos del batch de validación\n",
    "    embed_user_batch, embed_movie_batch, y_batch = data\n",
    "    # Copio el batch al dispositivo donde entreno la red neuronal\n",
    "    embed_user_batch = embed_user_batch.to(device).int()\n",
    "    embed_movie_batch = embed_movie_batch.to(device).int()\n",
    "    y_batch = y_batch.to(device).float().reshape(-1, 1)\n",
    "\n",
    "    # Paso forward\n",
    "    nnet_output = nnnetWithEmbeddings(embed_user_batch, embed_movie_batch)\n",
    "    y_batch_hat = nnet_output\n",
    "    \n",
    "    # Calculo el loss\n",
    "    loss = loss_function(nnet_output, y_batch)\n",
    "\n",
    "    # En validación no hago backpropagation!!\n",
    "\n",
    "    # Almaceno los valores reales y mis predicciones para cálcular las métricas\n",
    "    valid_epoch_y += list(y_batch.detach().cpu().numpy())\n",
    "    valid_epoch_y_hat += list(y_batch_hat.detach().cpu().numpy())\n",
    "    # Acumulo la loss del batch\n",
    "    valid_epoch_loss = valid_epoch_loss + loss.item()\n",
    "\n",
    "  # Calculo la media de la loss\n",
    "  valid_epoch_loss = valid_epoch_loss / n_valid\n",
    "  # Almaceno la loss de la epoch para graficar\n",
    "  valid_loss_by_epoch.append(valid_epoch_loss)\n",
    "  # Cálculo la métrica de la epoch\n",
    "  valid_mse = mean_squared_error(valid_epoch_y, valid_epoch_y_hat)\n",
    "\n",
    "  ############################################\n",
    "  ## Impresión de resultados por epoch\n",
    "  ############################################\n",
    "  print(f\" Epoch {epoch} | \" \\\n",
    "        f\"Train/Valid loss: {epoch_loss:.3f} / {valid_epoch_loss:.3f} | \" \\\n",
    "        f\"Train/Valid mse: {train_mse:.3f} / {valid_mse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000262C949E750>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
